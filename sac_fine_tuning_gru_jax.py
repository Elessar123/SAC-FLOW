#!/usr/bin/env python3
"""
SACç®—æ³• + JAX FlowMLP Actorçš„æ•´åˆç‰ˆæœ¬ + åˆ†ç¦»çš„SDE/ODEé‡‡æ · + åˆ†ç¦»çš„Flow/Gateç½‘ç»œä¼˜åŒ– + Poly-Tanhå˜æ¢
- ä½¿ç”¨JAX/Flaxå®ç°çš„FlowMLPç½‘ç»œæ¶æ„
- æ”¯æŒä»PyTorchæ£€æŸ¥ç‚¹åŠ è½½é¢„è®­ç»ƒæƒé‡å¹¶è½¬æ¢ä¸ºJAX
- åˆ†ç¦»çš„SDE Actorï¼ˆç”¨äºè®­ç»ƒï¼‰å’ŒODE Actorï¼ˆç”¨äºç¯å¢ƒäº¤äº’ï¼‰
- åˆ†ç¦»çš„Flowç½‘ç»œå’ŒGateç½‘ç»œå‚æ•°ä¼˜åŒ–ï¼Œæ”¯æŒä¸åŒå­¦ä¹ ç‡å’Œå†»ç»“ç­–ç•¥
- ã€æ–°å¢ã€‘Poly-Tanhå˜æ¢æ›¿ä»£ç¡¬è£å‰ªï¼Œå¹¶æ­£ç¡®è®¡ç®—log probability
- æ”¯æŒå¹¶è¡Œç¯å¢ƒè®­ç»ƒ
"""
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '3'
import random
import time
import logging
import math
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any
import numpy as np
import torch  # ä»…ç”¨äºåŠ è½½PyTorchæ£€æŸ¥ç‚¹
import tyro
from torch.utils.tensorboard import SummaryWriter
# test
# JAXç›¸å…³å¯¼å…¥
import jax
import jax.numpy as jnp
import flax
import flax.linen as nn
import optax
from flax.training.train_state import TrainState
import gymnasium as gym

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

# å¯¼å…¥make_asyncå’Œcleanrl buffer
try:
    from env.gym_utils import make_async
except ImportError:
    log.error("æ— æ³•å¯¼å…¥make_asyncå‡½æ•°ï¼Œè¯·ç¡®ä¿env.gym_utilsæ¨¡å—å¯ç”¨")
    raise

try:
    from cleanrl_utils.buffers import ReplayBuffer
except ImportError:
    log.error("æ— æ³•å¯¼å…¥cleanrl_utilsï¼Œè¯·å®‰è£…cleanrl")
    raise


@dataclass
class Args:
    exp_name: str = "sac_flowmlp_jax_separated_opt_poly_tanh"
    """the name of this experiment"""
    seed: int = 1
    """seed of the experiment"""
    torch_deterministic: bool = True
    """if toggled, `torch.backends.cudnn.deterministic=False`"""
    track: bool = False
    """if toggled, this experiment will be tracked with Weights and Biases"""
    wandb_project_name: str = "sac-flowmlp-jax-separated-opt-poly-tanh"
    """the wandb's project name"""
    wandb_entity: str = None
    """the entity (team) of wandb's project"""
    capture_video: bool = False
    """whether to capture videos of the agent performances"""
    save_model: bool = True
    """whether to save model into the `runs/{run_name}` folder"""

    # Environment
    env_id: str = "Walker2d-v2"
    """the id of the environment"""
    num_envs: int = 1
    """the number of parallel game environments"""
    max_episode_steps: int = 1000
    """maximum steps per episode"""
    
    # Algorithm specific arguments
    total_timesteps: int = 1000000
    """total timesteps of the experiments"""
    policy_lr: float = 3e-4
    """the learning rate of the policy network optimizer (deprecated, use flow_lr)"""
    flow_lr: float = 3e-5 
    """the learning rate of the flow network optimizer"""
    gate_lr: float = 1e-3
    """the learning rate of the gate network optimizer"""
    q_lr: float = 1e-3
    """the learning rate of the Q network network optimizer"""
    buffer_size: int = int(1e6)
    """the replay memory buffer size"""
    gamma: float = 0.99
    """the discount factor gamma"""
    tau: float = 0.005
    """target smoothing coefficient (default: 0.005)"""
    batch_size: int = 256
    """the batch size of sample from the reply memory"""
    alpha: float = 0.2
    """Entropy regularization coefficient."""
    autotune: bool = True
    """automatic tuning of the entropy coefficient"""
    learning_starts: int = 50000
    """timestep to start learning"""
    
    # ç½‘ç»œå†»ç»“å’Œå­¦ä¹ ç‡è°ƒåº¦å‚æ•°
    flow_freeze_steps: int = 250000
    """number of steps to freeze flow network training (0 = no freezing)"""
    gate_freeze_steps: int = 0
    """number of steps to freeze gate network training (0 = no freezing)"""
    flow_lr_schedule: str = "constant"
    """learning rate schedule for flow network: constant, linear_decay, cosine_decay"""
    gate_lr_schedule: str = "constant"
    """learning rate schedule for gate network: constant, linear_decay, cosine_decay"""
    flow_lr_warmup_steps: int = 400000
    """number of warmup steps for flow network learning rate"""
    gate_lr_warmup_steps: int = 100000
    """number of warmup steps for gate network learning rate"""
    flow_lr_decay_factor: float = 0.1
    """final learning rate factor for flow network decay schedules"""
    gate_lr_decay_factor: float = 0.1
    """final learning rate factor for gate network decay schedules"""
    
    # FlowMLPç½‘ç»œå‚æ•°
    load_pretrained: bool = True
    """whether to load pretrained weights for FlowMLP"""
    checkpoint_path: str = "pretrained_network/state_80_Walker2d.pt"
    """path to the pre-trained FlowMLP checkpoint (only used if load_pretrained=True)"""
    normalization_path: str = "data/gym/walker2d-medium-v2/normalization.npz"
    """path to normalization file for wrapper"""
    inference_steps: int = 4
    """number of inference steps for flow matching"""
    horizon_steps: int = 4
    """number of horizon steps the network outputs"""
    cond_steps: int = 1
    """number of observation steps"""
    act_steps: int = 4
    """number of action steps"""
    denoised_clip_value: float = 100
    """clip intermediate actions during inference"""
    
    # FlowMLPæ¶æ„å‚æ•°
    mlp_dims: List[int] = field(default_factory=lambda: [512, 512, 512])
    """MLP dimensions for FlowMLP"""
    time_dim: int = 16
    """time embedding dimension"""
    residual_style: bool = True
    """whether to use residual connections"""
    activation_type: str = "ReLU"
    """activation function type"""
    use_layernorm: bool = False
    """whether to use layer normalization"""
    
    # SDEé‡‡æ ·å’Œé—¨æ§ç½‘ç»œå‚æ•°
    sde_sigma: float = 0.5
    """noise strength for SDE sampling during training"""
    gate_hidden_dim: int = 128
    """hidden dimension for gate network"""

    # ã€æ–°å¢ã€‘Poly-Tanhå˜æ¢å‚æ•°
    use_poly_squash: bool = True
    """æ˜¯å¦ä½¿ç”¨ tanh(poly(x)) ä½œä¸ºæœ€ç»ˆçš„åŠ¨ä½œå‹ç¼©å‡½æ•°ï¼Œæ›¿ä»£ç¡¬è£å‰ª"""
    poly_order: int = 5
    """å½“ use_poly_squash=True æ—¶ï¼Œä½¿ç”¨å¤šé¡¹å¼çš„é˜¶æ•° (å»ºè®®ä¸ºå¥‡æ•°)"""

    def __post_init__(self):
        if self.mlp_dims is None:
            self.mlp_dims = [512, 512, 512]
        # ä¿æŒå‘åå…¼å®¹æ€§
        if hasattr(self, 'policy_lr') and self.flow_lr == 3e-4:
            self.flow_lr = self.policy_lr


# ==================== ã€æ–°å¢ã€‘Poly-Tanhå˜æ¢å‡½æ•° ====================

def poly_squash_transform(x, order):
    """
    åº”ç”¨ tanh(poly(x)) å˜æ¢
    poly(x) = x + x^3/3 + x^5/5 + ...
    """
    # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œå…ˆå¯¹è¾“å…¥xè¿›è¡Œè£å‰ªï¼Œé¿å…poly(x)è¿‡å¤§
    x = jnp.clip(x, -5.0, 5.0) 
    
    poly_x = jnp.zeros_like(x)
    for i in range(1, order + 1, 2):
        poly_x += (x**i) / i
    
    return jnp.tanh(poly_x)

def poly_derivative(x, order):
    """
    è®¡ç®—å¤šé¡¹å¼çš„å¯¼æ•°: d/dx [x + x^3/3 + x^5/5 + ...]
    = 1 + x^2 + x^4 + ...
    """
    x = jnp.clip(x, -5.0, 5.0)
    
    poly_deriv = jnp.zeros_like(x)
    for i in range(1, order + 1, 2):
        poly_deriv += x**(i-1)
    
    return poly_deriv

def poly_tanh_log_prob_correction(x, order):
    """
    è®¡ç®— tanh(poly(x)) å˜æ¢çš„logæ¦‚ç‡ä¿®æ­£é¡¹
    log|âˆ‚y/âˆ‚x| = log|âˆ‚tanh(poly(x))/âˆ‚x|
    = log|(1 - tanhÂ²(poly(x))) * poly'(x)|
    """
    x = jnp.clip(x, -5.0, 5.0)
    
    # è®¡ç®— poly(x)
    poly_x = jnp.zeros_like(x)
    for i in range(1, order + 1, 2):
        poly_x += (x**i) / i
    
    # è®¡ç®— poly'(x)
    poly_deriv = poly_derivative(x, order)
    
    # è®¡ç®— tanh(poly(x))
    tanh_poly_x = jnp.tanh(poly_x)
    
    # è®¡ç®—é›…å¯æ¯”è¡Œåˆ—å¼çš„ç»å¯¹å€¼
    jacobian = (1 - tanh_poly_x**2) * poly_deriv
    
    # è¿”å›logæ¦‚ç‡ä¿®æ­£é¡¹ï¼Œæ·»åŠ å°çš„epsiloné¿å…log(0)
    return jnp.log(jnp.abs(jacobian) + 1e-6)

# åˆ›å»ºJITç¼–è¯‘çš„ç‰¹å®šé˜¶æ•°ç‰ˆæœ¬
def create_poly_squash_jit(order):
    """åˆ›å»ºæŒ‡å®šé˜¶æ•°çš„JITç¼–è¯‘ç‰ˆæœ¬"""
    @jax.jit
    def _poly_squash_jit(x):
        return poly_squash_transform(x, order)
    return _poly_squash_jit

def create_poly_log_prob_correction_jit(order):
    """åˆ›å»ºæŒ‡å®šé˜¶æ•°çš„logæ¦‚ç‡ä¿®æ­£JITç¼–è¯‘ç‰ˆæœ¬"""
    @jax.jit
    def _poly_log_prob_correction_jit(x):
        return poly_tanh_log_prob_correction(x, order)
    return _poly_log_prob_correction_jit


# ==================== JAX/Flax FlowMLP Implementation ====================

class SinusoidalPosEmbFlax(nn.Module):
    dim: int
    
    def __call__(self, x):
        half_dim = self.dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = jnp.exp(jnp.arange(half_dim) * -emb)
        emb = x[:, None] * emb[None, :]
        emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], axis=-1)
        return emb


class MLPFlax(nn.Module):
    dim_list: List[int]
    activation_type: str = "Tanh"
    out_activation_type: str = "Identity"
    use_layernorm: bool = False
    
    def get_activation(self, activation_type):
        if activation_type == "Tanh":
            return nn.tanh
        elif activation_type == "ReLU":
            return nn.relu
        elif activation_type == "ELU":
            return nn.elu
        elif activation_type == "Mish":
            return lambda x: x * jnp.tanh(nn.softplus(x))
        elif activation_type == "GELU":
            return nn.gelu
        elif activation_type == "Sigmoid":
            return nn.sigmoid
        elif activation_type == "SiLU":
            return nn.swish
        elif activation_type == "Softplus":
            return nn.softplus
        else:  # Identity
            return lambda x: x
    
    @nn.compact
    def __call__(self, x, training=True):
        num_layer = len(self.dim_list) - 1
        for idx in range(num_layer):
            x = nn.Dense(self.dim_list[idx + 1])(x)
            
            if self.use_layernorm and idx < num_layer - 1:
                x = nn.LayerNorm(epsilon=1e-06)(x)
            
            activation_fn = (
                self.get_activation(self.activation_type)
                if idx != num_layer - 1
                else self.get_activation(self.out_activation_type)
            )
            x = activation_fn(x)
        
        return x


class TwoLayerPreActivationResNetLinearFlax(nn.Module):
    hidden_dim: int
    activation_type: str = "ReLU"
    use_layernorm: bool = False
    
    def get_activation(self, activation_type):
        if activation_type == "Tanh":
            return nn.tanh
        elif activation_type == "ReLU":
            return nn.relu
        elif activation_type == "ELU":
            return nn.elu
        elif activation_type == "Mish":
            return lambda x: x * jnp.tanh(nn.softplus(x))
        elif activation_type == "GELU":
            return nn.gelu
        elif activation_type == "Sigmoid":
            return nn.sigmoid
        elif activation_type == "SiLU":
            return nn.swish
        elif activation_type == "Softplus":
            return nn.softplus
        else:  # Identity
            return lambda x: x
    
    @nn.compact
    def __call__(self, x, training=True):
        x_input = x
        activation_fn = self.get_activation(self.activation_type)
        
        # First layer with pre-activation
        if self.use_layernorm:
            x = nn.LayerNorm(epsilon=1e-06)(x)
        x = activation_fn(x)
        x = nn.Dense(self.hidden_dim, name='l1')(x)
        
        # Second layer with pre-activation
        if self.use_layernorm:
            x = nn.LayerNorm(epsilon=1e-06)(x)
        x = activation_fn(x)
        x = nn.Dense(self.hidden_dim, name='l2')(x)
        
        return x + x_input


class ResidualMLPFlax(nn.Module):
    dim_list: List[int]
    activation_type: str = "ReLU"
    out_activation_type: str = "Identity"
    use_layernorm: bool = False
    
    def get_activation(self, activation_type):
        if activation_type == "Tanh":
            return nn.tanh
        elif activation_type == "ReLU":
            return nn.relu
        elif activation_type == "ELU":
            return nn.elu
        elif activation_type == "Mish":
            return lambda x: x * jnp.tanh(nn.softplus(x))
        elif activation_type == "GELU":
            return nn.gelu
        elif activation_type == "Sigmoid":
            return nn.sigmoid
        elif activation_type == "SiLU":
            return nn.swish
        elif activation_type == "Softplus":
            return nn.softplus
        else:  # Identity
            return lambda x: x
    
    @nn.compact
    def __call__(self, x, training=True):
        hidden_dim = self.dim_list[1]
        num_hidden_layers = len(self.dim_list) - 3
        assert num_hidden_layers % 2 == 0, f"num_hidden_layers must be even, got {num_hidden_layers}"
        
        # First linear layer
        x = nn.Dense(hidden_dim)(x)
        
        # Residual blocks
        num_residual_blocks = num_hidden_layers // 2
        for i in range(num_residual_blocks):
            x = TwoLayerPreActivationResNetLinearFlax(
                hidden_dim=hidden_dim,
                activation_type=self.activation_type,
                use_layernorm=self.use_layernorm,
                name=f'residual_block_{i}'
            )(x, training=training)
        
        # Final linear layer
        x = nn.Dense(self.dim_list[-1])(x)
        
        # Final activation
        activation_fn = self.get_activation(self.out_activation_type)
        x = activation_fn(x)
        
        return x


class FlowMLPFlax(nn.Module):
    horizon_steps: int
    action_dim: int
    cond_dim: int
    time_dim: int = 16
    mlp_dims: List[int] = None
    cond_mlp_dims: Optional[List[int]] = None
    activation_type: str = "ReLU"
    out_activation_type: str = "Identity"
    use_layernorm: bool = False
    residual_style: bool = False
    
    def setup(self):
        mlp_dims = list(self.mlp_dims) if self.mlp_dims is not None else [256, 256]
        act_dim_total = self.action_dim * self.horizon_steps
        
        # Time embedding layers
        self.sinusoidal_emb = SinusoidalPosEmbFlax(self.time_dim)
        self.time_dense1 = nn.Dense(self.time_dim * 2)
        self.time_dense2 = nn.Dense(self.time_dim)
        
        # Condition encoder
        if self.cond_mlp_dims:
            cond_mlp_dims_list = list(self.cond_mlp_dims)
            self.cond_mlp = MLPFlax(
                dim_list=[self.cond_dim] + cond_mlp_dims_list,
                activation_type=self.activation_type,
                out_activation_type="Identity",
            )
            cond_enc_dim = self.cond_mlp_dims[-1]
        else:
            cond_enc_dim = self.cond_dim
            
        input_dim = self.time_dim + self.action_dim * self.horizon_steps + cond_enc_dim
        
        # Main MLP
        if self.residual_style:
            self.mlp_mean = ResidualMLPFlax(
                dim_list=[input_dim] + mlp_dims + [act_dim_total],
                activation_type=self.activation_type,
                out_activation_type=self.out_activation_type,
                use_layernorm=self.use_layernorm,
            )
        else:
            self.mlp_mean = MLPFlax(
                dim_list=[input_dim] + mlp_dims + [act_dim_total],
                activation_type=self.activation_type,
                out_activation_type=self.out_activation_type,
                use_layernorm=self.use_layernorm,
            )
    
    def __call__(self, action, time, cond, training=True):
        """
        **Args**:
            action: (B, Ta, Da)
            time: (B,) or scalar, diffusion step
            cond: dict with key state; more recent obs at the end
                    state: (B, To, Do)
        **Outputs**:
            velocity: (B, Ta, Da)
        """
        B, Ta, Da = action.shape

        # flatten action chunk
        action = action.reshape(B, -1)

        # flatten obs history
        state = cond["state"].reshape(B, -1)

        # obs encoder
        if hasattr(self, "cond_mlp"):
            cond_emb = self.cond_mlp(state, training=training)
        else:
            cond_emb = state
        
        # time encoder
        if jnp.ndim(time) == 0:  # scalar
            time = jnp.ones((B, 1)) * time
        elif time.shape == (B,):
            time = time.reshape(B, 1)
        
        time_emb = self.sinusoidal_emb(time)
        time_emb = self.time_dense1(time_emb)
        time_emb = time_emb * jnp.tanh(nn.softplus(time_emb))  # Mish activation
        time_emb = self.time_dense2(time_emb)
        time_emb = time_emb.reshape(B, self.time_dim)
        
        # velocity head
        vel_feature = jnp.concatenate([action, time_emb, cond_emb], axis=-1)
        vel = self.mlp_mean(vel_feature, training=training)
        
        return vel.reshape(B, Ta, Da)


# ==================== é—¨æ§ç½‘ç»œå®ç° ====================

class GateNetworkFlax(nn.Module):
    """é—¨æ§ç½‘ç»œ - ç”¨äºè°ƒåˆ¶SDEé‡‡æ ·çš„æ¼‚ç§»é¡¹"""
    action_dim: int
    horizon_steps: int
    hidden_dim: int = 128
    
    @nn.compact
    def __call__(self, flow_input):
        # ç¬¬ä¸€å±‚ï¼šhidden layer
        x = nn.Dense(self.hidden_dim)(flow_input)
        x = nn.swish(x)
        
        # ç¬¬äºŒå±‚ï¼šè¾“å‡ºå±‚ï¼Œå…³é”®çš„åˆå§‹åŒ–
        x = nn.Dense(
            self.action_dim * self.horizon_steps,
            kernel_init=nn.initializers.zeros,  # æƒé‡åˆå§‹åŒ–ä¸º0
            bias_init=nn.initializers.constant(5.0)  # åç½®åˆå§‹åŒ–ä¸º5.0
        )(x)
        
        # Sigmoidæ¿€æ´»
        x = nn.sigmoid(x)
        
        # é‡å¡‘ä¸º (B, horizon_steps, action_dim)
        return x.reshape(-1, self.horizon_steps, self.action_dim)


# ==================== SDE Actor (ç”¨äºè®­ç»ƒ) ====================

class FlowMLPActorSDE(nn.Module):
    """JAX FlowMLP Actor - SDEç‰ˆæœ¬ï¼Œç”¨äºè®­ç»ƒï¼Œæ”¯æŒPoly-Tanhå˜æ¢"""
    obs_dim: int
    action_dim: int
    cond_steps: int = 1
    inference_steps: int = 4
    horizon_steps: int = 4
    denoised_clip_value: float = 3.0
    mlp_dims: List[int] = None
    time_dim: int = 16
    residual_style: bool = True
    activation_type: str = "ReLU"
    use_layernorm: bool = False
    sde_sigma: float = 0.01
    gate_hidden_dim: int = 128
    use_poly_squash: bool = True  # ã€æ–°å¢ã€‘
    poly_order: int = 5  # ã€æ–°å¢ã€‘
    
    def setup(self):
        # è®¡ç®—æ¡ä»¶ç»´åº¦
        cond_dim = self.obs_dim * self.cond_steps
        
        # FlowMLPæ¨¡å‹å‚æ•°
        model_params = {
            'horizon_steps': self.horizon_steps,
            'action_dim': self.action_dim,
            'cond_dim': cond_dim,
            'time_dim': self.time_dim,
            'mlp_dims': self.mlp_dims or [512, 512, 512],
            'cond_mlp_dims': None,
            'residual_style': self.residual_style,
            'activation_type': self.activation_type,
            'use_layernorm': self.use_layernorm,
            'out_activation_type': "Identity",
        }
        
        # åˆ›å»ºFlowMLPå®ä¾‹
        self.flow_network = FlowMLPFlax(**model_params)
        
        # åˆ›å»ºé—¨æ§ç½‘ç»œï¼ˆåªåœ¨SDEç‰ˆæœ¬ä¸­ä½¿ç”¨ï¼‰
        self.gate_network = GateNetworkFlax(
            action_dim=self.action_dim,
            horizon_steps=self.horizon_steps,
            hidden_dim=self.gate_hidden_dim
        )
        
        # ã€æ–°å¢ã€‘åˆ›å»ºJITç¼–è¯‘çš„polyå˜æ¢å‡½æ•°
        if self.use_poly_squash:
            self.poly_squash_jit = create_poly_squash_jit(self.poly_order)
            self.poly_log_prob_correction_jit = create_poly_log_prob_correction_jit(self.poly_order)
    
    def sample_first_point(self, B: int, key):
        """é‡‡æ ·åˆå§‹ç‚¹å¹¶è®¡ç®—log probability"""
        xt = jax.random.normal(key, (B, self.horizon_steps * self.action_dim))
        log_prob = jax.scipy.stats.norm.logpdf(xt, 0, 1).sum(axis=-1)
        xt = xt.reshape(B, self.horizon_steps, self.action_dim)
        return xt, log_prob
    
    def forward_step(self, xt, t, cond):
        """å•æ­¥å‰å‘ä¼ æ’­"""
        obs = cond["state"]
        obs_adjusted = obs
        if obs_adjusted.ndim == 2:
            obs_adjusted = jnp.expand_dims(obs_adjusted, axis=1)
        
        cond_adjusted = {"state": obs_adjusted}
        
        if t.ndim == 2:
            t_1d = t.squeeze(-1)
        else:
            t_1d = t
        
        velocity_output = self.flow_network(xt, t_1d, cond_adjusted, training=True)
        return velocity_output
    
    def compute_gate_values(self, obs_flat, xt_flat):
        """è®¡ç®—é—¨æ§å€¼ z"""
        gate_input = jnp.concatenate([obs_flat, xt_flat], axis=-1)
        z = self.gate_network(gate_input)
        return z
    
    def __call__(self, obs, key):
        """SDEé‡‡æ ·æ–¹æ³• - è·å–åŠ¨ä½œå’Œlog probabilityï¼Œæ”¯æŒPoly-Tanhå˜æ¢"""
        if obs.ndim == 1:
            obs = jnp.expand_dims(obs, axis=0)
            single_obs = True
        else:
            single_obs = False
            
        B = obs.shape[0]
        
        # æ„é€ æ¡ä»¶å­—å…¸
        if obs.ndim == 2:
            cond = {"state": jnp.expand_dims(obs, axis=1)}
        else:
            cond = {"state": obs}
        
        # é‡‡æ ·åˆå§‹ç‚¹å¹¶è·å–åˆå§‹log probability
        key, sample_key = jax.random.split(key)
        xt, log_prob = self.sample_first_point(B, sample_key)
        
        # SDEé‡‡æ ·å¾ªç¯
        dt = 1.0 / self.inference_steps
        time_steps = jnp.linspace(0, 1 - dt, self.inference_steps)
        
        for i in range(self.inference_steps):
            t_scalar = time_steps[i]
            t_tensor = jnp.full((B,), t_scalar)
            
            # ç”Ÿæˆéšæœºå™ªå£°
            key, noise_key = jax.random.split(key)
            
            # è®¡ç®—é€Ÿåº¦åœº
            u_t = self.forward_step(xt, t_tensor, cond)
            
            # ä½¿ç”¨å¹³æ»‘çš„æ¡ä»¶å¤„ç†
            epsilon = 1e-8
            t_safe = jnp.maximum(t_scalar, epsilon)
            
            # è®¡ç®—æƒé‡å› å­ï¼ˆå¹³æ»‘è¿‡æ¸¡ï¼‰
            sde_weight = nn.sigmoid((t_scalar - 0.01) * 100)
            
            # è®¡ç®—åˆ†æ•°å‡½æ•°
            s_t = (t_safe * u_t - xt) / jnp.maximum(1 - t_safe, epsilon)
            
            # è®¡ç®—SDEæ¼‚ç§»é¡¹
            drift_coef = ((1/t_safe - 1 + (self.sde_sigma**2)/2) * s_t + 
                         (1/t_safe) * xt)
            
            # åº”ç”¨é—¨æ§ç½‘ç»œ
            obs_flat = cond["state"].reshape(B, -1)
            xt_flat = xt.reshape(B, -1)
            z = self.compute_gate_values(obs_flat, xt_flat)
            drift_coef = z * drift_coef
            
            # ç”Ÿæˆå™ªå£°å’Œæ‰©æ•£é¡¹
            noise = jax.random.normal(noise_key, xt.shape)
            diffusion_coef = self.sde_sigma * jnp.sqrt(dt) * noise
            
            # ç»„åˆç¡®å®šæ€§å’Œéšæœºæ€§æ›´æ–°
            deterministic_update = xt + u_t * dt
            stochastic_update = xt + drift_coef * dt + diffusion_coef
            
            # å¹³æ»‘æ’å€¼
            xt = (1 - sde_weight) * deterministic_update + sde_weight * stochastic_update
            
            # æ›´æ–°log probability
            noise_log_prob = sde_weight * jax.scipy.stats.norm.logpdf(
                noise.reshape(B, -1), 0, self.sde_sigma * jnp.sqrt(dt)
            ).sum(axis=-1)
            log_prob = log_prob + noise_log_prob
            
            # ä¸­é—´åŠ¨ä½œè£å‰ª
            if i < self.inference_steps - 1:
                xt = jnp.clip(xt, -self.denoised_clip_value, self.denoised_clip_value)
            else:
                # ã€ä¿®æ”¹ã€‘æœ€åä¸€æ­¥ï¼šåº”ç”¨Poly-Tanhå˜æ¢æˆ–ä¼ ç»Ÿtanh
                xt_flat = xt.reshape(B, -1)
                
                if self.use_poly_squash:
                    # ä½¿ç”¨Poly-Tanhå˜æ¢
                    xt_squashed = self.poly_squash_jit(xt_flat)
                    
                    # ã€å…³é”®ã€‘è®¡ç®—Poly-Tanhå˜æ¢çš„logæ¦‚ç‡ä¿®æ­£
                    poly_log_prob_correction = self.poly_log_prob_correction_jit(xt_flat)
                    log_prob = log_prob - poly_log_prob_correction.sum(axis=-1)
                else:
                    # ä¼ ç»Ÿtanhæ¿€æ´»
                    xt_squashed = jnp.tanh(xt_flat)
                    
                    # ä¼ ç»Ÿtanhçš„logæ¦‚ç‡ä¿®æ­£
                    log_prob = log_prob - jnp.log(1 - xt_squashed**2 + 1e-6).sum(axis=-1)
                
                # é‡å¡‘
                xt = xt_squashed.reshape(B, self.horizon_steps, self.action_dim)
        
        # åªè¿”å›ç¬¬0æ­¥çš„åŠ¨ä½œ
        action = xt[:, 0, :]
        
        if single_obs:
            return action.squeeze(0), log_prob.squeeze(0)
        else:
            return action, log_prob


# ==================== ODE Actor (ç”¨äºç¯å¢ƒäº¤äº’) ====================

class FlowMLPActorODE(nn.Module):
    """JAX FlowMLP Actor - ODEç‰ˆæœ¬ï¼Œç”¨äºç¯å¢ƒäº¤äº’ï¼Œæ”¯æŒPoly-Tanhå˜æ¢"""
    obs_dim: int
    action_dim: int
    cond_steps: int = 1
    inference_steps: int = 4
    horizon_steps: int = 4
    denoised_clip_value: float = 3.0
    mlp_dims: List[int] = None
    time_dim: int = 16
    residual_style: bool = True
    activation_type: str = "ReLU"
    use_layernorm: bool = False
    gate_hidden_dim: int = 128
    use_poly_squash: bool = True  # ã€æ–°å¢ã€‘
    poly_order: int = 5  # ã€æ–°å¢ã€‘
    
    def setup(self):
        # è®¡ç®—æ¡ä»¶ç»´åº¦
        cond_dim = self.obs_dim * self.cond_steps
        
        # FlowMLPæ¨¡å‹å‚æ•°
        model_params = {
            'horizon_steps': self.horizon_steps,
            'action_dim': self.action_dim,
            'cond_dim': cond_dim,
            'time_dim': self.time_dim,
            'mlp_dims': self.mlp_dims or [512, 512, 512],
            'cond_mlp_dims': None,
            'residual_style': self.residual_style,
            'activation_type': self.activation_type,
            'use_layernorm': self.use_layernorm,
            'out_activation_type': "Identity",
        }
        
        # åˆ›å»ºFlowMLPå®ä¾‹
        self.flow_network = FlowMLPFlax(**model_params)
        
        # åˆ›å»ºé—¨æ§ç½‘ç»œï¼ˆODEç‰ˆæœ¬ä¹Ÿéœ€è¦é—¨æ§ç½‘ç»œï¼ï¼‰
        self.gate_network = GateNetworkFlax(
            action_dim=self.action_dim,
            horizon_steps=self.horizon_steps,
            hidden_dim=self.gate_hidden_dim
        )
        
        # ã€æ–°å¢ã€‘åˆ›å»ºJITç¼–è¯‘çš„polyå˜æ¢å‡½æ•°
        if self.use_poly_squash:
            self.poly_squash_jit = create_poly_squash_jit(self.poly_order)
    
    def sample_first_point(self, B: int, key):
        """é‡‡æ ·åˆå§‹ç‚¹"""
        xt = jax.random.normal(key, (B, self.horizon_steps * self.action_dim))
        xt = xt.reshape(B, self.horizon_steps, self.action_dim)
        return xt
    
    def forward_step(self, xt, t, cond):
        """å•æ­¥å‰å‘ä¼ æ’­"""
        obs = cond["state"]
        obs_adjusted = obs
        if obs_adjusted.ndim == 2:
            obs_adjusted = jnp.expand_dims(obs_adjusted, axis=1)
        
        cond_adjusted = {"state": obs_adjusted}
        
        if t.ndim == 2:
            t_1d = t.squeeze(-1)
        else:
            t_1d = t
        
        velocity_output = self.flow_network(xt, t_1d, cond_adjusted, training=False)
        return velocity_output
    
    def compute_gate_values(self, obs_flat, xt_flat):
        """è®¡ç®—é—¨æ§å€¼ z"""
        gate_input = jnp.concatenate([obs_flat, xt_flat], axis=-1)
        z = self.gate_network(gate_input)
        return z
    
    def __call__(self, obs, key):
        """ODEé‡‡æ ·æ–¹æ³• - ç¡®å®šæ€§æ¨ç†ï¼Œæ”¯æŒPoly-Tanhå˜æ¢"""
        if obs.ndim == 1:
            obs = jnp.expand_dims(obs, axis=0)
            single_obs = True
        else:
            single_obs = False
            
        B = obs.shape[0]
        
        # æ„é€ æ¡ä»¶å­—å…¸
        if obs.ndim == 2:
            cond = {"state": jnp.expand_dims(obs, axis=1)}
        else:
            cond = {"state": obs}
        
        # é‡‡æ ·åˆå§‹ç‚¹
        key, sample_key = jax.random.split(key)
        xt = self.sample_first_point(B, sample_key)
        
        # ODEé‡‡æ ·å¾ªç¯
        dt = 1.0 / self.inference_steps
        time_steps = jnp.linspace(0, 1 - dt, self.inference_steps)
        
        for i in range(self.inference_steps):
            t_scalar = time_steps[i]
            t_tensor = jnp.full((B,), t_scalar)
            
            # è®¡ç®—é€Ÿåº¦åœº
            u_t = self.forward_step(xt, t_tensor, cond)
            
            # è®¡ç®—ODEæ¼‚ç§»é¡¹
            drift_coef = u_t
            
            # åº”ç”¨é—¨æ§ç½‘ç»œ
            obs_flat = cond["state"].reshape(B, -1)
            xt_flat = xt.reshape(B, -1)
            z = self.compute_gate_values(obs_flat, xt_flat)
            drift_coef = z * drift_coef
            
            # ODEæ›´æ–°ï¼šç›´æ¥æ›´æ–°ï¼Œæ— å¹³æ»‘æ’å€¼
            xt = xt + drift_coef * dt
            
            # ä¸­é—´åŠ¨ä½œè£å‰ª
            if i < self.inference_steps - 1:
                xt = jnp.clip(xt, -self.denoised_clip_value, self.denoised_clip_value)
            else:
                # ã€ä¿®æ”¹ã€‘æœ€åä¸€æ­¥ï¼šåº”ç”¨Poly-Tanhå˜æ¢æˆ–ä¼ ç»Ÿtanh
                xt_flat = xt.reshape(B, -1)
                
                if self.use_poly_squash:
                    # ä½¿ç”¨Poly-Tanhå˜æ¢
                    xt_squashed = self.poly_squash_jit(xt_flat)
                else:
                    # ä¼ ç»Ÿtanhæ¿€æ´»
                    xt_squashed = jnp.tanh(xt_flat)
                
                xt = xt_squashed.reshape(B, self.horizon_steps, self.action_dim)
        
        # åªè¿”å›ç¬¬0æ­¥çš„åŠ¨ä½œ
        action = xt[:, 0, :]
        
        return action.squeeze(0) if single_obs else action


# ==================== Parameter Conversion Functions ====================

def extract_torch_mlp_params(state_dict, prefix):
    """Extract MLP parameters from PyTorch state dict"""
    params = {}
    layer_idx = 0
    
    # Find all module indices
    module_indices = set()
    for key in state_dict.keys():
        if key.startswith(f"{prefix}.moduleList."):
            parts = key.split(".")
            if len(parts) >= 3:
                try:
                    module_idx = int(parts[2])
                    module_indices.add(module_idx)
                except ValueError:
                    continue
    
    for module_idx in sorted(module_indices):
        # Extract linear layer parameters
        weight_key = f"{prefix}.moduleList.{module_idx}.linear_1.weight"
        bias_key = f"{prefix}.moduleList.{module_idx}.linear_1.bias"
        
        if weight_key in state_dict and bias_key in state_dict:
            dense_name = f"Dense_{layer_idx}"
            params[dense_name] = {
                'kernel': jnp.array(state_dict[weight_key].detach().cpu().numpy().T),
                'bias': jnp.array(state_dict[bias_key].detach().cpu().numpy())
            }
            layer_idx += 1
    
    return params


def extract_torch_residual_mlp_params(state_dict, prefix):
    """Extract ResidualMLP parameters from PyTorch state dict"""
    params = {}
    dense_idx = 0
    
    # Find all layer indices
    layer_indices = set()
    for key in state_dict.keys():
        if key.startswith(f"{prefix}.layers."):
            parts = key.split(".")
            if len(parts) >= 3:
                try:
                    layer_idx = int(parts[2])
                    layer_indices.add(layer_idx)
                except ValueError:
                    continue
    
    residual_block_idx = 0
    for layer_idx in sorted(layer_indices):
        # Check if it's a Linear layer
        weight_key = f"{prefix}.layers.{layer_idx}.weight"
        bias_key = f"{prefix}.layers.{layer_idx}.bias"
        
        if weight_key in state_dict and bias_key in state_dict:
            dense_name = f"Dense_{dense_idx}"
            params[dense_name] = {
                'kernel': jnp.array(state_dict[weight_key].detach().cpu().numpy().T),
                'bias': jnp.array(state_dict[bias_key].detach().cpu().numpy())
            }
            dense_idx += 1
        else:
            # Check if it's a TwoLayerPreActivationResNetLinear
            l1_weight_key = f"{prefix}.layers.{layer_idx}.l1.weight"
            l1_bias_key = f"{prefix}.layers.{layer_idx}.l1.bias"
            l2_weight_key = f"{prefix}.layers.{layer_idx}.l2.weight"
            l2_bias_key = f"{prefix}.layers.{layer_idx}.l2.bias"
            
            if (l1_weight_key in state_dict and l1_bias_key in state_dict and 
                l2_weight_key in state_dict and l2_bias_key in state_dict):
                
                # Create residual block parameters
                residual_block_name = f"residual_block_{residual_block_idx}"
                params[residual_block_name] = {
                    'l1': {
                        'kernel': jnp.array(state_dict[l1_weight_key].detach().cpu().numpy().T),
                        'bias': jnp.array(state_dict[l1_bias_key].detach().cpu().numpy())
                    },
                    'l2': {
                        'kernel': jnp.array(state_dict[l2_weight_key].detach().cpu().numpy().T),
                        'bias': jnp.array(state_dict[l2_bias_key].detach().cpu().numpy())
                    }
                }
                residual_block_idx += 1
    
    return params


def torch_to_jax_params(torch_state_dict, jax_model, sample_input):
    """Convert PyTorch model parameters to JAX/Flax format"""
    
    # Initialize JAX model to get parameter structure
    key = jax.random.PRNGKey(0)
    action, time, cond = sample_input
    jax_params = jax_model.init(key, action, time, cond)
    
    log.info("Converting PyTorch parameters to JAX...")
    
    # Create a new parameter dictionary
    new_params = {}
    
    # Time embedding parameters
    new_params['time_dense1'] = {
        'kernel': jnp.array(torch_state_dict['time_embedding.1.weight'].detach().cpu().numpy().T),
        'bias': jnp.array(torch_state_dict['time_embedding.1.bias'].detach().cpu().numpy())
    }
    new_params['time_dense2'] = {
        'kernel': jnp.array(torch_state_dict['time_embedding.3.weight'].detach().cpu().numpy().T),
        'bias': jnp.array(torch_state_dict['time_embedding.3.bias'].detach().cpu().numpy())
    }
    
    # Condition MLP parameters (if exists)
    cond_mlp_keys = [k for k in torch_state_dict.keys() if k.startswith('cond_mlp')]
    if cond_mlp_keys:
        new_params['cond_mlp'] = extract_torch_mlp_params(torch_state_dict, 'cond_mlp')
    
    # Main MLP parameters - check if it's residual style
    residual_keys = [k for k in torch_state_dict.keys() if 'mlp_mean.layers' in k and '.l1.' in k]
    if residual_keys:
        new_params['mlp_mean'] = extract_torch_residual_mlp_params(torch_state_dict, 'mlp_mean')
    else:
        new_params['mlp_mean'] = extract_torch_mlp_params(torch_state_dict, 'mlp_mean')
    
    # Replace the parameters in the JAX model
    jax_params['params'] = new_params
    
    return new_params  # åªè¿”å›FlowMLPçš„å‚æ•°ï¼Œä¸åŒ…å«å¤–å±‚ç»“æ„


# ==================== Learning Rate Schedulers ====================

def create_lr_schedule(base_lr: float, schedule_type: str, total_steps: int, 
                      warmup_steps: int = 0, decay_factor: float = 0.1):
    """åˆ›å»ºJAXå…¼å®¹çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def warmup_schedule(step):
        # ä½¿ç”¨JAXæ¡ä»¶æ“ä½œè€Œä¸æ˜¯Python if
        warmup_factor = jnp.where(
            (warmup_steps > 0) & (step < warmup_steps),
            (step + 1) / warmup_steps,
            1.0
        )
        return base_lr * warmup_factor
    
    def constant_schedule(step):
        return warmup_schedule(step)
    
    def linear_decay_schedule(step):
        base_rate = warmup_schedule(step)
        
        # ä½¿ç”¨JAXæ¡ä»¶æ“ä½œ
        decay_steps = total_steps - warmup_steps
        progress = jnp.maximum(0.0, (step - warmup_steps) / decay_steps)
        progress = jnp.minimum(progress, 1.0)
        
        # å½“step < warmup_stepsæ—¶ï¼Œprogressä¸ºè´Ÿæ•°ï¼Œä¼šè¢«clampåˆ°0ï¼Œæ‰€ä»¥è¡°å‡å› å­ä¸º1
        decay_multiplier = 1.0 - progress * (1.0 - decay_factor)
        
        return base_rate * decay_multiplier
    
    def cosine_decay_schedule(step):
        base_rate = warmup_schedule(step)
        
        # ä½¿ç”¨JAXæ¡ä»¶æ“ä½œ
        decay_steps = total_steps - warmup_steps
        progress = jnp.maximum(0.0, (step - warmup_steps) / decay_steps)
        progress = jnp.minimum(progress, 1.0)
        
        cosine_decay = 0.5 * (1.0 + jnp.cos(jnp.pi * progress))
        decay_multiplier = decay_factor + (1.0 - decay_factor) * cosine_decay
        
        return base_rate * decay_multiplier
    
    if schedule_type == "constant":
        return constant_schedule
    elif schedule_type == "linear_decay":
        return linear_decay_schedule
    elif schedule_type == "cosine_decay":
        return cosine_decay_schedule
    else:
        raise ValueError(f"Unknown schedule type: {schedule_type}")


# ==================== Critic Network ====================

class QNetwork(nn.Module):
    """SAC Criticç½‘ç»œ - JAXç‰ˆæœ¬"""
    
    @nn.compact
    def __call__(self, x: jnp.ndarray, a: jnp.ndarray):
        x = jnp.concatenate([x, a], -1)
        x = nn.Dense(256)(x)
        x = nn.relu(x)
        x = nn.Dense(256)(x)
        x = nn.relu(x)
        x = nn.Dense(1)(x)
        return x


# ==================== Entropy Coefficient ====================

class EntropyCoef(nn.Module):
    """Learnable entropy coefficient for SAC"""
    ent_coef_init: float = 1.0

    @nn.compact
    def __call__(self) -> jnp.ndarray:
        log_ent_coef = self.param("log_ent_coef", init_fn=lambda key: jnp.full((), jnp.log(self.ent_coef_init)))
        return jnp.exp(log_ent_coef)


# ==================== Training State with Separate Networks ====================

class SeparatedTrainState:
    """åˆ†ç¦»çš„è®­ç»ƒçŠ¶æ€ï¼ŒåŒ…å«Flowç½‘ç»œå’ŒGateç½‘ç»œçš„ç‹¬ç«‹ä¼˜åŒ–å™¨"""
    def __init__(self, flow_state: TrainState, gate_state: TrainState):
        self.flow_state = flow_state
        self.gate_state = gate_state
    
    @property
    def params(self):
        """ç»„åˆå‚æ•°"""
        return {
            'params': {
                'flow_network': self.flow_state.params,
                'gate_network': self.gate_state.params
            }
        }
    
    def replace(self, flow_state=None, gate_state=None):
        """æ›¿æ¢çŠ¶æ€"""
        return SeparatedTrainState(
            flow_state=flow_state if flow_state is not None else self.flow_state,
            gate_state=gate_state if gate_state is not None else self.gate_state
        )

# æ³¨å†ŒSeparatedTrainStateä¸ºJAX pytree
def _separated_train_state_tree_flatten(state):
    """å°†SeparatedTrainStateå±•å¹³ä¸ºJAXèƒ½å¤„ç†çš„æ ¼å¼"""
    children = (state.flow_state, state.gate_state)
    aux_data = None
    return children, aux_data

def _separated_train_state_tree_unflatten(aux_data, children):
    """ä»å±•å¹³çš„æ ¼å¼é‡æ„SeparatedTrainState"""
    flow_state, gate_state = children
    return SeparatedTrainState(flow_state, gate_state)

# æ³¨å†Œpytree
jax.tree_util.register_pytree_node(
    SeparatedTrainState,
    _separated_train_state_tree_flatten,
    _separated_train_state_tree_unflatten
)


class TrainState(TrainState):
    target_params: flax.core.FrozenDict


# ==================== Utility Functions ====================

def reset_env_all(venv, num_envs, verbose=False, options_venv=None, **kwargs):
    """é‡ç½®æ‰€æœ‰ç¯å¢ƒ"""
    if options_venv is None:
        options_venv = [
            {k: v for k, v in kwargs.items()} for _ in range(num_envs)
        ]
    
    obs_venv = venv.reset_arg(options_list=options_venv)
    
    if isinstance(obs_venv, list):
        obs_venv = {
            key: np.stack([obs_venv[i][key] for i in range(num_envs)])
            for key in obs_venv[0].keys()
        }
    
    return obs_venv


def process_obs(obs_venv, main_obs_key=None):
    """å¤„ç†è§‚å¯Ÿå€¼ï¼Œç¡®ä¿å½¢çŠ¶æ­£ç¡®"""
    if isinstance(obs_venv, dict):
        obs = obs_venv[main_obs_key]
    else:
        obs = obs_venv
    
    # å¦‚æœè§‚å¯Ÿå€¼æœ‰å¤šäº2ä¸ªç»´åº¦ï¼Œéœ€è¦å±•å¹³é™¤ç¬¬ä¸€ä¸ªç»´åº¦å¤–çš„æ‰€æœ‰ç»´åº¦
    if obs.ndim > 2:
        obs = obs.reshape(obs.shape[0], -1)
    
    return obs


def load_pretrained_flowmlp_params(checkpoint_path, flowmlp_config, sample_input):
    """åŠ è½½é¢„è®­ç»ƒFlowMLPå‚æ•°å¹¶è½¬æ¢ä¸ºJAXæ ¼å¼"""
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    log.info(f"Loading pretrained FlowMLP from: {checkpoint_path}")
    
    # åŠ è½½PyTorchæ£€æŸ¥ç‚¹
    device = torch.device("cpu")
    try:
        checkpoint_data = torch.load(checkpoint_path, map_location=device, weights_only=False)
    except Exception as e:
        log.error(f"Failed to load checkpoint: {e}")
        raise
    
    # è·å–state_dict
    if 'model' in checkpoint_data:
        state_dict = checkpoint_data['model']
        log.info("Using 'model' key from checkpoint")
    elif 'ema' in checkpoint_data:
        state_dict = checkpoint_data['ema']
        log.info("Using 'ema' key from checkpoint")
    else:
        state_dict = checkpoint_data
        log.info("Using direct state_dict from checkpoint")
    
    log.info(f"Found {len(state_dict)} parameter keys in checkpoint")
    
    # ä¿®æ­£state_dicté”®åï¼ˆç§»é™¤'network.'å‰ç¼€ï¼‰
    corrected_state_dict = {}
    for k, v in state_dict.items():
        new_key = k.replace('network.', '', 1) if k.startswith('network.') else k
        corrected_state_dict[new_key] = v
    
    log.info(f"Key examples: {list(corrected_state_dict.keys())[:5]}...")
    
    # åˆ›å»ºç‹¬ç«‹çš„FlowMLPå®ä¾‹ç”¨äºå‚æ•°è½¬æ¢
    try:
        flowmlp_module = FlowMLPFlax(**flowmlp_config)
        log.info(f"Created FlowMLP with config: {flowmlp_config}")
    except Exception as e:
        log.error(f"Failed to create FlowMLP module: {e}")
        raise
    
    # è½¬æ¢å‚æ•°ä»PyTorchåˆ°JAX
    try:
        jax_params = torch_to_jax_params(corrected_state_dict, flowmlp_module, sample_input)
        log.info("Pretrained FlowMLP parameters converted to JAX successfully!")
        return jax_params
    except Exception as e:
        log.error(f"Failed to convert parameters: {e}")
        raise


if __name__ == "__main__":
    args = tyro.cli(Args)
    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}_sde{args.sde_sigma}_flow_lr{args.flow_lr}_gate_lr{args.gate_lr}_poly{args.poly_order if args.use_poly_squash else 'off'}__{int(time.time())}"
    
    if args.track:
        import wandb
        wandb.init(
            project=args.wandb_project_name,
            entity=args.wandb_entity,
            sync_tensorboard=True,
            config=vars(args),
            name=run_name,
            monitor_gym=True,
            save_code=True,
        )
    
    writer = SummaryWriter(f"runs/{run_name}")
    writer.add_text(
        "hyperparameters",
        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
    )

    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)  # ç”¨äºç¯å¢ƒ
    key = jax.random.PRNGKey(args.seed)
    key, actor_sde_key, actor_ode_key, qf1_key, qf2_key, alpha_key, action_key = jax.random.split(key, 7)

    # ç¯å¢ƒè®¾ç½®
    log.info(f"åˆ›å»º {args.num_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ: {args.env_id}")
    log.info(f"SDEé‡‡æ ·é…ç½®: sigma = {args.sde_sigma}")
    log.info(f"é—¨æ§ç½‘ç»œé…ç½®: hidden_dim = {args.gate_hidden_dim}")
    log.info(f"Flowç½‘ç»œå­¦ä¹ ç‡: {args.flow_lr}, è°ƒåº¦: {args.flow_lr_schedule}, å†»ç»“æ­¥æ•°: {args.flow_freeze_steps}")
    log.info(f"Gateç½‘ç»œå­¦ä¹ ç‡: {args.gate_lr}, è°ƒåº¦: {args.gate_lr_schedule}, å†»ç»“æ­¥æ•°: {args.gate_freeze_steps}")
    # ã€æ–°å¢ã€‘Poly-Tanhé…ç½®æ—¥å¿—
    if args.use_poly_squash:
        log.info(f"ğŸš€ Poly-Tanhå˜æ¢: å¯ç”¨ï¼Œé˜¶æ•° = {args.poly_order}")
    else:
        log.info(f"âŒ Poly-Tanhå˜æ¢: ç¦ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿtanh")
    
    # ç¯å¢ƒwrapperé…ç½®
    wrappers_config = {
        "mujoco_locomotion_lowdim": {
            "normalization_path": args.normalization_path
        },
        "multi_step": {
            "n_obs_steps": args.cond_steps,
            "n_action_steps": args.act_steps,
            "max_episode_steps": args.max_episode_steps,
            "reset_within_step": True
        }
    }
    
    envs = make_async(
        args.env_id,
        env_type=None,
        num_envs=args.num_envs,
        asynchronous=True,
        max_episode_steps=args.max_episode_steps,
        wrappers=wrappers_config,
        robomimic_env_cfg_path=None,
        shape_meta=None,
        use_image_obs=False,
        render=args.capture_video,
        render_offscreen=args.capture_video,
        obs_dim=11,
        action_dim=3,
    )
    
    envs.seed([args.seed + i for i in range(args.num_envs)])
    
    # è·å–ç¯å¢ƒè§„æ ¼
    dummy_obs = reset_env_all(envs, args.num_envs, verbose=False)
    if isinstance(dummy_obs, dict):
        main_obs_key = list(dummy_obs.keys())[0]
        obs_shape = dummy_obs[main_obs_key].shape
        if len(obs_shape) > 2:
            obs_flat_dim = np.prod(obs_shape[1:])
        else:
            obs_flat_dim = obs_shape[-1]
    else:
        obs_shape = dummy_obs.shape
        if len(obs_shape) > 2:
            obs_flat_dim = np.prod(obs_shape[1:])
        else:
            obs_flat_dim = obs_shape[-1]
        main_obs_key = None
    
    action_dim = envs.action_space.shape[-1]
    
    # åˆ›å»ºobservation_spaceå’Œaction_spaceå¯¹è±¡ç”¨äºreplay buffer
    observation_space = gym.spaces.Box(
        low=np.full(obs_flat_dim, -np.inf, dtype=np.float32),
        high=np.full(obs_flat_dim, np.inf, dtype=np.float32),
        shape=(obs_flat_dim,), 
        dtype=np.float32
    )
    action_space = gym.spaces.Box(
        low=-1, high=1, shape=(action_dim,), dtype=np.float32
    )
    
    # è·å–å®é™…çš„obs_dim
    obs_dim = obs_flat_dim // args.cond_steps if obs_flat_dim % args.cond_steps == 0 else obs_flat_dim
    
    # åˆ›å»ºSDE Actorï¼ˆç”¨äºè®­ç»ƒï¼‰
    log.info("åˆ›å»ºSDE Actorï¼ˆç”¨äºè®­ç»ƒï¼ŒåŒ…å«é—¨æ§ç½‘ç»œå’ŒPoly-Tanhå˜æ¢ï¼‰")
    actor_sde = FlowMLPActorSDE(
        obs_dim=obs_dim,
        action_dim=action_dim,
        cond_steps=args.cond_steps,
        inference_steps=args.inference_steps,
        horizon_steps=args.horizon_steps,
        denoised_clip_value=args.denoised_clip_value,
        mlp_dims=args.mlp_dims,
        time_dim=args.time_dim,
        residual_style=args.residual_style,
        activation_type=args.activation_type,
        use_layernorm=args.use_layernorm,
        sde_sigma=args.sde_sigma,
        gate_hidden_dim=args.gate_hidden_dim,
        use_poly_squash=args.use_poly_squash,  # ã€æ–°å¢ã€‘
        poly_order=args.poly_order  # ã€æ–°å¢ã€‘
    )
    
    # åˆ›å»ºODE Actorï¼ˆç”¨äºç¯å¢ƒäº¤äº’ï¼‰
    log.info("åˆ›å»ºODE Actorï¼ˆç”¨äºç¯å¢ƒäº¤äº’ï¼Œä¹ŸåŒ…å«é—¨æ§ç½‘ç»œå’ŒPoly-Tanhå˜æ¢ï¼‰")
    actor_ode = FlowMLPActorODE(
        obs_dim=obs_dim,
        action_dim=action_dim,
        cond_steps=args.cond_steps,
        inference_steps=args.inference_steps,
        horizon_steps=args.horizon_steps,
        denoised_clip_value=args.denoised_clip_value,
        mlp_dims=args.mlp_dims,
        time_dim=args.time_dim,
        residual_style=args.residual_style,
        activation_type=args.activation_type,
        use_layernorm=args.use_layernorm,
        gate_hidden_dim=args.gate_hidden_dim,
        use_poly_squash=args.use_poly_squash,  # ã€æ–°å¢ã€‘
        poly_order=args.poly_order  # ã€æ–°å¢ã€‘
    )
    
    # åˆ›å»ºåˆå§‹è§‚æµ‹ç”¨äºç½‘ç»œåˆå§‹åŒ–
    obs_venv = reset_env_all(envs, args.num_envs)
    obs = process_obs(obs_venv, main_obs_key)
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    flow_lr_schedule = create_lr_schedule(
        base_lr=args.flow_lr,
        schedule_type=args.flow_lr_schedule,
        total_steps=args.total_timesteps,
        warmup_steps=args.flow_lr_warmup_steps,
        decay_factor=args.flow_lr_decay_factor
    )
    
    gate_lr_schedule = create_lr_schedule(
        base_lr=args.gate_lr,
        schedule_type=args.gate_lr_schedule,
        total_steps=args.total_timesteps,
        warmup_steps=args.gate_lr_warmup_steps,
        decay_factor=args.gate_lr_decay_factor
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    flow_tx = optax.adam(learning_rate=flow_lr_schedule)
    gate_tx = optax.adam(learning_rate=gate_lr_schedule)
    
    # åˆå§‹åŒ–actorå‚æ•°
    if args.load_pretrained and os.path.exists(args.checkpoint_path):
        log.info("å‡†å¤‡åŠ è½½é¢„è®­ç»ƒFlowMLPæƒé‡...")
        
        # åˆ›å»ºæ ·ä¾‹è¾“å…¥ç”¨äºå‚æ•°è½¬æ¢
        batch_size = 1
        sample_action = jnp.zeros((batch_size, args.horizon_steps, action_dim))
        sample_time = jnp.zeros((batch_size,))
        sample_cond = {"state": jnp.zeros((batch_size, args.cond_steps, obs_dim))}
        sample_input = (sample_action, sample_time, sample_cond)
        
        # å‡†å¤‡FlowMLPé…ç½®
        flowmlp_config = {
            'horizon_steps': args.horizon_steps,
            'action_dim': action_dim,
            'cond_dim': obs_dim * args.cond_steps,
            'time_dim': args.time_dim,
            'mlp_dims': args.mlp_dims,
            'cond_mlp_dims': None,
            'residual_style': args.residual_style,
            'activation_type': args.activation_type,
            'use_layernorm': args.use_layernorm,
            'out_activation_type': "Identity",
        }
        
        try:
            # åŠ è½½é¢„è®­ç»ƒå‚æ•°
            pretrained_flowmlp_params = load_pretrained_flowmlp_params(
                args.checkpoint_path, flowmlp_config, sample_input
            )
            
            # åˆå§‹åŒ–SDE Actorå¹¶åˆ†ç¦»Flowå’ŒGateç½‘ç»œå‚æ•°
            initial_params_sde = actor_sde.init(actor_sde_key, obs, action_key)
            
            # åˆ›å»ºåˆ†ç¦»çš„è®­ç»ƒçŠ¶æ€
            flow_params = pretrained_flowmlp_params  # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            gate_params = initial_params_sde['params']['gate_network']  # éšæœºåˆå§‹åŒ–é—¨æ§ç½‘ç»œ
            
            flow_state_sde = TrainState.create(
                apply_fn=lambda params, *args, **kwargs: actor_sde.flow_network.apply(params, *args, **kwargs),
                params=flow_params,
                target_params=flow_params,
                tx=flow_tx,
            )
            
            gate_state_sde = TrainState.create(
                apply_fn=lambda params, *args, **kwargs: actor_sde.gate_network.apply(params, *args, **kwargs),
                params=gate_params,
                target_params=gate_params,
                tx=gate_tx,
            )
            
            actor_sde_state = SeparatedTrainState(flow_state_sde, gate_state_sde)
            
            # åˆå§‹åŒ–ODE Actorå¹¶åŠ è½½ç›¸åŒæƒé‡
            initial_params_ode = actor_ode.init(actor_ode_key, obs, action_key)
            
            flow_state_ode = TrainState.create(
                apply_fn=lambda params, *args, **kwargs: actor_ode.flow_network.apply(params, *args, **kwargs),
                params=flow_params,
                target_params=flow_params,
                tx=flow_tx,
            )
            
            gate_state_ode = TrainState.create(
                apply_fn=lambda params, *args, **kwargs: actor_ode.gate_network.apply(params, *args, **kwargs),
                params=gate_params,
                target_params=gate_params,
                tx=gate_tx,
            )
            
            actor_ode_state = SeparatedTrainState(flow_state_ode, gate_state_ode)
            
            log.info("SDEå’ŒODE Actorå·²åŠ è½½é¢„è®­ç»ƒFlowMLPæƒé‡ï¼ˆé—¨æ§ç½‘ç»œéšæœºåˆå§‹åŒ–ï¼‰")
            
        except Exception as e:
            log.error(f"é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥: {e}")
            log.info("å›é€€åˆ°éšæœºåˆå§‹åŒ–...")
            
            # å›é€€åˆ°éšæœºåˆå§‹åŒ–
            initial_params_sde = actor_sde.init(actor_sde_key, obs, action_key)
            initial_params_ode = actor_ode.init(actor_ode_key, obs, action_key)
            
            # SDE Actoråˆ†ç¦»çŠ¶æ€
            flow_state_sde = TrainState.create(
                apply_fn=lambda params, *args, **kwargs: actor_sde.flow_network.apply(params, *args, **kwargs),
                params=initial_params_sde['params']['flow_network'],
                target_params=initial_params_sde['params']['flow_network'],
                tx=flow_tx,
            )
            
            gate_state_sde = TrainState.create(
                apply_fn=lambda params, *args, **kwargs: actor_sde.gate_network.apply(params, *args, **kwargs),
                params=initial_params_sde['params']['gate_network'],
                target_params=initial_params_sde['params']['gate_network'],
                tx=gate_tx,
            )
            
            actor_sde_state = SeparatedTrainState(flow_state_sde, gate_state_sde)
            
            # ODE Actoråˆ†ç¦»çŠ¶æ€
            flow_state_ode = TrainState.create(
                apply_fn=lambda params, *args, **kwargs: actor_ode.flow_network.apply(params, *args, **kwargs),
                params=initial_params_ode['params']['flow_network'],
                target_params=initial_params_ode['params']['flow_network'],
                tx=flow_tx,
            )
            
            gate_state_ode = TrainState.create(
                apply_fn=lambda params, *args, **kwargs: actor_ode.gate_network.apply(params, *args, **kwargs),
                params=initial_params_ode['params']['gate_network'],
                target_params=initial_params_ode['params']['gate_network'],
                tx=gate_tx,
            )
            
            actor_ode_state = SeparatedTrainState(flow_state_ode, gate_state_ode)
            
            log.info("ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„Actorç½‘ç»œ")
    else:
        # éšæœºåˆå§‹åŒ–
        initial_params_sde = actor_sde.init(actor_sde_key, obs, action_key)
        initial_params_ode = actor_ode.init(actor_ode_key, obs, action_key)
        
        # SDE Actoråˆ†ç¦»çŠ¶æ€
        flow_state_sde = TrainState.create(
            apply_fn=lambda params, *args, **kwargs: actor_sde.flow_network.apply(params, *args, **kwargs),
            params=initial_params_sde['params']['flow_network'],
            target_params=initial_params_sde['params']['flow_network'],
            tx=flow_tx,
        )
        
        gate_state_sde = TrainState.create(
            apply_fn=lambda params, *args, **kwargs: actor_sde.gate_network.apply(params, *args, **kwargs),
            params=initial_params_sde['params']['gate_network'],
            target_params=initial_params_sde['params']['gate_network'],
            tx=gate_tx,
        )
        
        actor_sde_state = SeparatedTrainState(flow_state_sde, gate_state_sde)
        
        # ODE Actoråˆ†ç¦»çŠ¶æ€
        flow_state_ode = TrainState.create(
            apply_fn=lambda params, *args, **kwargs: actor_ode.flow_network.apply(params, *args, **kwargs),
            params=initial_params_ode['params']['flow_network'],
            target_params=initial_params_ode['params']['flow_network'],
            tx=flow_tx,
        )
        
        gate_state_ode = TrainState.create(
            apply_fn=lambda params, *args, **kwargs: actor_ode.gate_network.apply(params, *args, **kwargs),
            params=initial_params_ode['params']['gate_network'],
            target_params=initial_params_ode['params']['gate_network'],
            tx=gate_tx,
        )
        
        actor_ode_state = SeparatedTrainState(flow_state_ode, gate_state_ode)
        
        log.info("ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„Actorç½‘ç»œ")
    
    # åˆ›å»ºCriticç½‘ç»œ
    qf = QNetwork()
    dummy_action = jnp.zeros((args.num_envs, action_dim))
    
    qf1_state = TrainState.create(
        apply_fn=qf.apply,
        params=qf.init(qf1_key, obs, dummy_action),
        target_params=qf.init(qf1_key, obs, dummy_action),
        tx=optax.adam(learning_rate=args.q_lr),
    )
    qf2_state = TrainState.create(
        apply_fn=qf.apply,
        params=qf.init(qf2_key, obs, dummy_action),
        target_params=qf.init(qf2_key, obs, dummy_action),
        tx=optax.adam(learning_rate=args.q_lr),
    )
    
    # Entropy coefficient
    if args.autotune:
        target_entropy = -np.prod((action_dim,)).astype(np.float32)
        entropy_coef = EntropyCoef(args.alpha)
        alpha_state = TrainState.create(
            apply_fn=entropy_coef.apply,
            params=entropy_coef.init(alpha_key),
            target_params=entropy_coef.init(alpha_key),
            tx=optax.adam(learning_rate=args.q_lr),
        )
    else:
        target_entropy = 0.0
        alpha_state = None
    
    # JITç¼–è¯‘
    actor_sde.apply = jax.jit(actor_sde.apply)
    actor_ode.apply = jax.jit(actor_ode.apply)
    qf.apply = jax.jit(qf.apply)

    # åˆ›å»ºreplay buffer
    rb = ReplayBuffer(
        args.buffer_size,
        observation_space,
        action_space,
        device="cpu",
        n_envs=args.num_envs,
        handle_timeout_termination=False,
    )
    
    # å®šä¹‰è®­ç»ƒå‡½æ•°
    @jax.jit
    def update_critic(
        actor_sde_state: SeparatedTrainState,
        qf1_state: TrainState,
        qf2_state: TrainState,
        alpha_state: TrainState,
        observations: jnp.ndarray,
        actions: jnp.ndarray,
        next_observations: jnp.ndarray,
        rewards: jnp.ndarray,
        terminations: jnp.ndarray,
        key: jnp.ndarray,
    ):
        key, sample_key = jax.random.split(key, 2)
        
        # ä½¿ç”¨SDE Actoré‡‡æ ·ä¸‹ä¸€æ­¥åŠ¨ä½œï¼ˆè®­ç»ƒæ—¶ï¼‰
        next_actions, next_log_prob = actor_sde.apply(actor_sde_state.params, next_observations, sample_key)
        
        # Get current alpha value
        if alpha_state is not None:
            alpha_value = entropy_coef.apply(alpha_state.params)
        else:
            alpha_value = args.alpha
        
        # Compute target Q values
        qf1_next_target = qf.apply(qf1_state.target_params, next_observations, next_actions).reshape(-1)
        qf2_next_target = qf.apply(qf2_state.target_params, next_observations, next_actions).reshape(-1)
        min_qf_next_target = jnp.minimum(qf1_next_target, qf2_next_target)
        next_q_value = (rewards + (1 - terminations) * args.gamma * 
                       (min_qf_next_target - alpha_value * next_log_prob)).reshape(-1)

        def mse_loss(params):
            qf_a_values = qf.apply(params, observations, actions).squeeze()
            return ((qf_a_values - next_q_value) ** 2).mean(), qf_a_values.mean()

        (qf1_loss_value, qf1_a_values), grads1 = jax.value_and_grad(mse_loss, has_aux=True)(qf1_state.params)
        (qf2_loss_value, qf2_a_values), grads2 = jax.value_and_grad(mse_loss, has_aux=True)(qf2_state.params)
        qf1_state = qf1_state.apply_gradients(grads=grads1)
        qf2_state = qf2_state.apply_gradients(grads=grads2)

        return (qf1_state, qf2_state), (qf1_loss_value, qf2_loss_value), (qf1_a_values, qf2_a_values), key

    def update_actor_and_alpha(
        actor_sde_state: SeparatedTrainState,
        actor_ode_state: SeparatedTrainState,
        qf1_state: TrainState,
        qf2_state: TrainState,
        alpha_state: TrainState,
        observations: jnp.ndarray,
        key: jnp.ndarray,
        flow_frozen: bool,
        gate_frozen: bool,
    ):
        key, sample_key = jax.random.split(key, 2)
        
        # ActoræŸå¤±å‡½æ•°
        def actor_loss_fn(flow_params, gate_params):
            # ç»„åˆå‚æ•°
            combined_params = {
                'params': {
                    'flow_network': flow_params,
                    'gate_network': gate_params
                }
            }
            
            # ä½¿ç”¨SDE Actorè¿›è¡Œè®­ç»ƒ
            actions, log_prob = actor_sde.apply(combined_params, observations, sample_key)
            
            qf1_pi = qf.apply(qf1_state.params, observations, actions)
            qf2_pi = qf.apply(qf2_state.params, observations, actions)
            min_qf_pi = jnp.minimum(qf1_pi, qf2_pi)
            
            if alpha_state is not None:
                alpha_value = entropy_coef.apply(alpha_state.params)
            else:
                alpha_value = args.alpha
            
            actor_loss = (alpha_value * log_prob - min_qf_pi).mean()
            return actor_loss, log_prob.mean()

        # åˆ†åˆ«è®¡ç®—Flowå’ŒGateç½‘ç»œçš„æ¢¯åº¦
        def flow_loss_fn(flow_params):
            loss, entropy = actor_loss_fn(flow_params, actor_sde_state.gate_state.params)
            return loss, entropy
        
        def gate_loss_fn(gate_params):
            loss, entropy = actor_loss_fn(actor_sde_state.flow_state.params, gate_params)
            return loss, entropy
        
        # è®¡ç®—æ¢¯åº¦
        (actor_loss_value, entropy), flow_grads = jax.value_and_grad(flow_loss_fn, has_aux=True)(actor_sde_state.flow_state.params)
        _, gate_grads = jax.value_and_grad(gate_loss_fn, has_aux=True)(actor_sde_state.gate_state.params)
        
        # ä½¿ç”¨JAXæ¡ä»¶æ“ä½œæ¥å¤„ç†å†»ç»“çŠ¶æ€
        def apply_flow_grads(state):
            return state.apply_gradients(grads=flow_grads)
        
        def keep_flow_state(state):
            return state
        
        def apply_gate_grads(state):
            return state.apply_gradients(grads=gate_grads)
        
        def keep_gate_state(state):
            return state
        
        # åº”ç”¨æ¢¯åº¦ï¼ˆä½¿ç”¨JAXæ¡ä»¶æ“ä½œï¼‰
        flow_state_sde_new = jax.lax.cond(
            flow_frozen,
            keep_flow_state,
            apply_flow_grads,
            actor_sde_state.flow_state
        )
        
        gate_state_sde_new = jax.lax.cond(
            gate_frozen,
            keep_gate_state,
            apply_gate_grads,
            actor_sde_state.gate_state
        )
        
        # æ›´æ–°SDE ActorçŠ¶æ€
        actor_sde_state = actor_sde_state.replace(
            flow_state=flow_state_sde_new,
            gate_state=gate_state_sde_new
        )
        
        # åŒæ­¥æ›´æ–°ODE Actorå‚æ•°
        flow_state_ode_new = actor_ode_state.flow_state.replace(params=flow_state_sde_new.params)
        gate_state_ode_new = actor_ode_state.gate_state.replace(params=gate_state_sde_new.params)
        
        actor_ode_state = actor_ode_state.replace(
            flow_state=flow_state_ode_new,
            gate_state=gate_state_ode_new
        )
        
        # Update alpha if autotune
        alpha_loss_value = 0.0
        if alpha_state is not None:
            def alpha_loss_fn(alpha_params):
                alpha_value = entropy_coef.apply(alpha_params)
                alpha_loss = (alpha_value * (-entropy - target_entropy)).mean()
                return alpha_loss
            
            alpha_loss_value, alpha_grads = jax.value_and_grad(alpha_loss_fn)(alpha_state.params)
            alpha_state = alpha_state.apply_gradients(grads=alpha_grads)

        # Update target networks
        qf1_state = qf1_state.replace(
            target_params=optax.incremental_update(qf1_state.params, qf1_state.target_params, args.tau)
        )
        qf2_state = qf2_state.replace(
            target_params=optax.incremental_update(qf2_state.params, qf2_state.target_params, args.tau)
        )

        return actor_sde_state, actor_ode_state, alpha_state, (qf1_state, qf2_state), actor_loss_value, alpha_loss_value, key
    
    # JITç¼–è¯‘æ›´æ–°å‡½æ•°ï¼Œæ ‡è®°å†»ç»“å‚æ•°ä¸ºé™æ€
    update_actor_and_alpha = jax.jit(update_actor_and_alpha, static_argnames=['flow_frozen', 'gate_frozen'])

    start_time = time.time()
    
    # è·Ÿè¸ªepisodeç»Ÿè®¡
    episode_returns = np.zeros(args.num_envs)
    episode_lengths = np.zeros(args.num_envs, dtype=int)
    completed_episodes = 0
    all_episode_returns = []
    
    log.info("å¼€å§‹SACè®­ç»ƒï¼ˆåˆ†ç¦»çš„Flow/Gateç½‘ç»œä¼˜åŒ–ç‰ˆæœ¬ + Poly-Tanhå˜æ¢ï¼‰")
    log.info("è®­ç»ƒç­–ç•¥: SDE Actorç”¨äºè®­ç»ƒï¼ŒODE Actorç”¨äºç¯å¢ƒäº¤äº’ï¼ŒFlowå’ŒGateç½‘ç»œåˆ†ç¦»ä¼˜åŒ–")
    
    for global_step in range(args.total_timesteps):
        # åŠ¨ä½œé€‰æ‹©
        if global_step < args.learning_starts:
            if args.load_pretrained and os.path.exists(args.checkpoint_path):
                # å¦‚æœåŠ è½½äº†é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨ODE Actoræ”¶é›†åˆå§‹æ•°æ®
                key, action_key = jax.random.split(key, 2)
                actions = actor_ode.apply(actor_ode_state.params, obs, action_key)
                actions = jax.device_get(actions)
                actions = np.array(actions, copy=True)
            else:
                # æ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œ
                actions = np.array([envs.action_space.sample() for _ in range(args.num_envs)])
                # å¤„ç†åŠ¨ä½œå½¢çŠ¶
                if actions.ndim > 2:
                    actions = actions.squeeze(1)
                if actions.ndim == 3:
                    actions = actions[:, 0, :]  # å–ç¬¬ä¸€ä¸ªåŠ¨ä½œæ­¥
        else:
            # æ­£å¸¸è®­ç»ƒé˜¶æ®µï¼šä½¿ç”¨ODE Actorè¿›è¡Œç¯å¢ƒäº¤äº’ï¼ˆç¡®å®šæ€§æ¨ç†ï¼‰
            key, action_key = jax.random.split(key, 2)
            actions = actor_ode.apply(actor_ode_state.params, obs, action_key)
            actions = jax.device_get(actions)
            actions = np.array(actions, copy=True)

        # æ‰§è¡ŒåŠ¨ä½œ
        next_obs_venv, rewards, terminations, truncations, infos = envs.step(actions)
        next_obs = process_obs(next_obs_venv, main_obs_key)

        # æ›´æ–°episodeç»Ÿè®¡
        episode_returns += rewards
        episode_lengths += 1

        # è®°å½•å®Œæˆçš„episodes
        for env_idx in range(args.num_envs):
            if terminations[env_idx] or truncations[env_idx]:
                all_episode_returns.append(episode_returns[env_idx])
                writer.add_scalar("charts/episodic_return", episode_returns[env_idx], global_step)
                writer.add_scalar("charts/episodic_length", episode_lengths[env_idx], global_step)
                completed_episodes += 1
                
                # æ¯20ä¸ªepisodeæ‰“å°ä¸€æ¬¡è¿›åº¦
                if completed_episodes % 20 == 0:
                    recent_returns = np.array(all_episode_returns[-20:])
                    poly_status = f"Poly{args.poly_order}" if args.use_poly_squash else "Tanh"
                    log.info(f"Episodes: {completed_episodes}, Recent 20 mean return: {np.mean(recent_returns):.2f} ({poly_status})")
                
                episode_returns[env_idx] = 0
                episode_lengths[env_idx] = 0

        # ä¿å­˜æ•°æ®åˆ°replay buffer
        real_next_obs = next_obs.copy()
        for idx, trunc in enumerate(truncations):
            if trunc and hasattr(infos, '__getitem__') and 'final_observation' in infos:
                final_obs_processed = process_obs(infos["final_observation"], main_obs_key)
                real_next_obs[idx] = final_obs_processed[idx]
        
        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
        obs = next_obs

        # è®­ç»ƒ
        if global_step > args.learning_starts:
            data = rb.sample(args.batch_size)
            
            # è½¬æ¢ä¸ºJAXæ•°ç»„
            observations = jnp.array(data.observations.numpy())
            actions = jnp.array(data.actions.numpy())
            next_observations = jnp.array(data.next_observations.numpy())
            rewards = jnp.array(data.rewards.flatten().numpy())
            terminations = jnp.array(data.dones.flatten().numpy())
            
            # æ›´æ–°criticï¼ˆä½¿ç”¨SDE Actorï¼‰
            (qf1_state, qf2_state), (qf1_loss_value, qf2_loss_value), (qf1_a_values, qf2_a_values), key = update_critic(
                actor_sde_state,
                qf1_state,
                qf2_state,
                alpha_state,
                observations,
                actions,
                next_observations,
                rewards,
                terminations,
                key,
            )

            # æ›´æ–°actorå’Œalphaï¼ˆåˆ†ç¦»ä¼˜åŒ–Flowå’ŒGateç½‘ç»œï¼‰
            key, actor_update_key = jax.random.split(key)
            
            # åœ¨JITå‡½æ•°å¤–è®¡ç®—å†»ç»“çŠ¶æ€
            flow_frozen = global_step < args.flow_freeze_steps
            gate_frozen = global_step < args.gate_freeze_steps
            
            actor_sde_state, actor_ode_state, alpha_state, (qf1_state, qf2_state), actor_loss_value, alpha_loss_value, key = update_actor_and_alpha(
                actor_sde_state,
                actor_ode_state,
                qf1_state,
                qf2_state,
                alpha_state,
                observations,
                actor_update_key,
                flow_frozen,
                gate_frozen,
            )

            # è®°å½•æŸå¤±
            if global_step % 100 == 0:
                writer.add_scalar("losses/qf1_loss", float(qf1_loss_value), global_step)
                writer.add_scalar("losses/qf2_loss", float(qf2_loss_value), global_step)
                writer.add_scalar("losses/qf1_values", float(qf1_a_values), global_step)
                writer.add_scalar("losses/qf2_values", float(qf2_a_values), global_step)
                writer.add_scalar("losses/actor_loss", float(actor_loss_value), global_step)
                
                # è®°å½•å­¦ä¹ ç‡
                current_flow_lr = flow_lr_schedule(global_step)
                current_gate_lr = gate_lr_schedule(global_step)
                writer.add_scalar("learning_rates/flow_lr", float(jax.device_get(current_flow_lr)), global_step)
                writer.add_scalar("learning_rates/gate_lr", float(jax.device_get(current_gate_lr)), global_step)
                
                # è®°å½•å†»ç»“çŠ¶æ€
                writer.add_scalar("training_status/flow_frozen", 1 if global_step < args.flow_freeze_steps else 0, global_step)
                writer.add_scalar("training_status/gate_frozen", 1 if global_step < args.gate_freeze_steps else 0, global_step)
                
                # ã€æ–°å¢ã€‘è®°å½•Poly-TanhçŠ¶æ€
                writer.add_scalar("training_status/use_poly_squash", 1 if args.use_poly_squash else 0, global_step)
                if args.use_poly_squash:
                    writer.add_scalar("training_status/poly_order", args.poly_order, global_step)
                
                if args.autotune:
                    current_alpha = entropy_coef.apply(alpha_state.params)
                    writer.add_scalar("losses/alpha", float(current_alpha), global_step)
                    writer.add_scalar("losses/alpha_loss", float(alpha_loss_value), global_step)
                else:
                    writer.add_scalar("losses/alpha", args.alpha, global_step)
                
                sps = int(global_step / (time.time() - start_time))
                writer.add_scalar("charts/SPS", sps, global_step)
                
                if global_step % 5000 == 0:
                    poly_status = f"Poly{args.poly_order}" if args.use_poly_squash else "Tanh"
                    print(f"Step {global_step}/{args.total_timesteps}, SPS: {sps}, Episodes: {completed_episodes} ({poly_status})")

    # ä¿å­˜æ¨¡å‹
    if args.save_model:
        model_path = f"runs/{run_name}/{args.exp_name}.cleanrl_model"
        with open(model_path, "wb") as f:
            f.write(
                flax.serialization.to_bytes([
                    actor_sde_state.flow_state.params,
                    actor_sde_state.gate_state.params,
                    actor_ode_state.flow_state.params,
                    actor_ode_state.gate_state.params,
                    qf1_state.params,
                    qf2_state.params,
                    alpha_state.params if alpha_state is not None else None,
                ])
            )
        print(f"Model saved to {model_path}")

    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    if len(all_episode_returns) > 0:
        final_returns = np.array(all_episode_returns)
        poly_status = f"Poly{args.poly_order}" if args.use_poly_squash else "Tanh"
        log.info(f"è®­ç»ƒå®Œæˆï¼æ€»episodes: {len(final_returns)}, "
               f"å¹³å‡å›æŠ¥: {np.mean(final_returns):.2f} Â± {np.std(final_returns):.2f} ({poly_status})")

    envs.close()
    writer.close()
    
    log.info("SAC FlowMLP JAXï¼ˆåˆ†ç¦»çš„Flow/Gateç½‘ç»œä¼˜åŒ– + Poly-Tanhå˜æ¢ï¼‰è®­ç»ƒå®Œæˆï¼")
    log.info("å…³é”®ç‰¹æ€§æ€»ç»“:")
    log.info("  1. SDE Actorç”¨äºè®­ç»ƒï¼ˆåŒ…å«é—¨æ§ç½‘ç»œå¢å¼ºæ¢ç´¢ï¼‰")
    log.info("  2. ODE Actorç”¨äºç¯å¢ƒäº¤äº’ï¼ˆç¡®å®šæ€§æ¨ç†ï¼‰")
    log.info("  3. Flowç½‘ç»œå’ŒGateç½‘ç»œåˆ†ç¦»ä¼˜åŒ–ï¼Œæ”¯æŒä¸åŒå­¦ä¹ ç‡å’Œå†»ç»“ç­–ç•¥")
    log.info("  4. è‡ªåŠ¨åŒæ­¥FlowMLPå‚æ•°ï¼Œä¿æŒä¸€è‡´æ€§")
    log.info("  5. å®Œå…¨é¿å…äº†æ¡ä»¶é€»è¾‘å¸¦æ¥çš„JAXç¼–è¯‘é—®é¢˜")
    if args.load_pretrained:
        log.info("  6. ä½¿ç”¨äº†é¢„è®­ç»ƒFlowMLPæƒé‡è¿›è¡Œfine-tuning")
    else:
        log.info("  6. ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„FlowMLPç½‘ç»œè¿›è¡Œè®­ç»ƒ")
    log.info(f"  7. SDEå‚æ•°: sigma = {args.sde_sigma}")
    log.info(f"  8. é—¨æ§ç½‘ç»œ: hidden_dim = {args.gate_hidden_dim}")
    log.info(f"  9. Flowç½‘ç»œ: lr = {args.flow_lr}, è°ƒåº¦ = {args.flow_lr_schedule}, å†»ç»“æ­¥æ•° = {args.flow_freeze_steps}")
    log.info(f"  10. Gateç½‘ç»œ: lr = {args.gate_lr}, è°ƒåº¦ = {args.gate_lr_schedule}, å†»ç»“æ­¥æ•° = {args.gate_freeze_steps}")
    # ã€æ–°å¢ã€‘Poly-Tanhç‰¹æ€§æ—¥å¿—
    if args.use_poly_squash:
        log.info(f"  11. âœ… Poly-Tanhå˜æ¢: å¯ç”¨ï¼Œé˜¶æ•° = {args.poly_order}")
        log.info(f"      - æ›¿ä»£ç¡¬è£å‰ªï¼Œæä¾›æ›´å¹³æ»‘çš„åŠ¨ä½œå‹ç¼©")
        log.info(f"      - æ­£ç¡®è®¡ç®—é›…å¯æ¯”è¡Œåˆ—å¼ï¼Œä¿æŒlogæ¦‚ç‡å‡†ç¡®æ€§")
        log.info(f"      - æ•°å€¼ç¨³å®šçš„å¤šé¡¹å¼å®ç° (è¾“å…¥è£å‰ªåˆ°[-5,5])")
        log.info(f"      - JITç¼–è¯‘ä¼˜åŒ–ï¼Œæé«˜è®¡ç®—æ•ˆç‡")
    else:
        log.info(f"  11. âŒ Poly-Tanhå˜æ¢: ç¦ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿtanhå‹ç¼©")
    
    log.info("\nğŸ”§ Poly-Tanhå˜æ¢çš„æ ¸å¿ƒæ”¹è¿›:")
    if args.use_poly_squash:
        log.info(f"  - å¤šé¡¹å¼å˜æ¢: poly(x) = x + xÂ³/3 + xâµ/5 + ... (é˜¶æ•°={args.poly_order})")
        log.info(f"  - æœ€ç»ˆè¾“å‡º: tanh(poly(x))ï¼Œæ¯”ç›´æ¥tanh(x)æ›´å¹³æ»‘")
        log.info(f"  - logæ¦‚ç‡ä¿®æ­£: æ­£ç¡®è®¡ç®—å˜æ¢çš„é›…å¯æ¯”è¡Œåˆ—å¼")
        log.info(f"  - é€‚ç”¨åœºæ™¯: SDEå’ŒODEé‡‡æ ·çš„æœ€ç»ˆåŠ¨ä½œå‹ç¼©")
        log.info(f"  - æ•°å€¼ç¨³å®šæ€§: è¾“å…¥è£å‰ª + epsilonä¿æŠ¤")
    else:
        log.info(f"  - ä½¿ç”¨ä¼ ç»Ÿtanh(x)å˜æ¢")
        log.info(f"  - é€‚åˆéœ€è¦å¼ºç¡¬çº¦æŸçš„åœºæ™¯")
    
    log.info(f"\nğŸ“Š è®­ç»ƒé…ç½®æ€»ç»“:")
    log.info(f"  - ç¯å¢ƒ: {args.env_id}")
    log.info(f"  - æ€»æ­¥æ•°: {args.total_timesteps}")
    log.info(f"  - æ‰¹é‡å¤§å°: {args.batch_size}")
    log.info(f"  - æ¨ç†æ­¥æ•°: {args.inference_steps}")
    log.info(f"  - æ—¶é—´èŒƒå›´: {args.horizon_steps}")
    log.info(f"  - æ¡ä»¶æ­¥æ•°: {args.cond_steps}")
    log.info(f"  - MLPå±‚æ•°: {args.mlp_dims}")
    log.info(f"  - æ¿€æ´»å‡½æ•°: {args.activation_type}")
    log.info(f"  - æ®‹å·®è¿æ¥: {args.residual_style}")
    
    if args.use_poly_squash:
        log.info(f"\nğŸ¯ å»ºè®®çš„Poly-Tanhè°ƒä¼˜ç­–ç•¥:")
        log.info(f"  1. é˜¶æ•°é€‰æ‹©: å¥‡æ•°é˜¶(3,5,7)é€šå¸¸æ•ˆæœæ›´å¥½")
        log.info(f"  2. å½“å‰é˜¶æ•° {args.poly_order}: {'é€‚ä¸­' if 3 <= args.poly_order <= 7 else 'å¯èƒ½éœ€è¦è°ƒæ•´'}")
        log.info(f"  3. å¦‚æœåŠ¨ä½œè¿‡äºä¿å®ˆï¼Œå¯é€‚å½“é™ä½é˜¶æ•°")
        log.info(f"  4. å¦‚æœåŠ¨ä½œä¸å¤Ÿå¹³æ»‘ï¼Œå¯é€‚å½“å¢åŠ é˜¶æ•°")
        log.info(f"  5. é…åˆSDE sigmaè°ƒä¼˜ï¼Œè·å¾—æ›´å¥½çš„æ¢ç´¢-åˆ©ç”¨å¹³è¡¡")